{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量的原理\n",
    "- 文本处理的最细颗粒度为单个词（字太小了），最原始的表示是采用对所有的词，建立词库，采用onehot向量来表示每个词。\n",
    "\n",
    "比如：有一份语料，总共包含“a”，“b”，“c”三个词，建立一个词库，那么就是一个[“a”，“b”，“c”]。当我要表示a的时候就是：[1,0,0]。这个是最原始的表达，显然当词海比较大的时候，这个向量是非常稀疏的。\n",
    "- 采用labelencode方式来给每一个词，一个编号。\n",
    "\n",
    "比如：还是上面的例子，当词库的建立方式是一个列表的时候，[“a”，“b”，“c”]，可以采用每个词对应的下标来表示该词。比如a就可以用0来表示。显然这样也是有缺点的，这些编号是无意义并且有差别的，比如苹果和香蕉还有人，分别的编码是1，1000，2。\n",
    "- 词向量表达（word2vec）\n",
    "\n",
    "通过训练语言模型，得到的中间产物来表示词。主要的模型是CBOW和Skip-gram。CBOW是建立语言模型，用某个词的上下文做输入，来预测这个词。Skip-gram与之相反。CBOW适用于小型语料库，Skip-gram适用大语料库。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "\n",
    "<img src=\"./img/CBOW.jpg\" style=\"zoom:50%\">\n",
    "\n",
    "#### Skip-gram\n",
    "<img src=\"./img/Skip-gram.jpg\" style=\"zoom:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当训练好这连个语言模型之后，会得到权重矩阵$W[N,V]$，当我们设定每个词的onehot表达为$x=[1,N]$时，就可以用这个权重矩阵和onehot向量相乘，得到词向量的表达，即：\n",
    "$$word2vec=X[1,N]\\times W[N,V]=vec[1,V]$$\n",
    "#### 下面用代码实现一下词向量的训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模块导入\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个句子生成器\n",
    "class MySentences(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname    \n",
    "    def __iter__(self):\n",
    "        for line in open(self.fname,'r'):            \n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "DataDir = \"./\"\n",
    "ModelDir = \"./\"\n",
    "#词频低于该值的舍去\n",
    "MIN_COUNT = 4\n",
    "# 需要预先安装 Cython 以支持并行\n",
    "CPU_NUM = 2 \n",
    "# 设置词向量维度\n",
    "VEC_SIZE = 20\n",
    "# 提取目标词上下文距离最长5个词\n",
    "CONTEXT_WINDOW = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vTrain(f_input, model_output):         \n",
    "\n",
    "    sentences = MySentences(DataDir+f_input)\n",
    "    # 这里是一个循环训练过程，做了２个操作，第一建立词库，第二用神经网络训练\n",
    "    # 也可以分开\n",
    "    #　model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "    #　model.build_vocab(some_sentences)  # 建立此表\n",
    "    #　model.train(other_sentences)  # 训练\n",
    "    w2v_model = word2vec.Word2Vec(sentences, \n",
    "                                  sg=0,# cbow,1=skip-gram\n",
    "                                  min_count = MIN_COUNT, \n",
    "                                  workers = CPU_NUM, \n",
    "                                  size = VEC_SIZE,\n",
    "                                  window = CONTEXT_WINDOW)\n",
    "    # 保存模型\n",
    "    w2v_model.save(ModelDir+model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_input = \"bioCorpus_5000.txt\"\n",
    "model_output = \"test_w2v_model\"\n",
    "w2vTrain(f_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01855507, -0.01266545,  0.00810266, -0.01430729,  0.01543698,\n",
       "       -0.01801598,  0.01267219,  0.00947818,  0.02281465, -0.01764751,\n",
       "        0.00306669,  0.00366936, -0.01334176, -0.00713251,  0.01816056,\n",
       "       -0.01962536,  0.01906137, -0.01963314, -0.01498264,  0.00227621],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练好的词向量模型才有效,初始的词向量是用初始化的参数矩阵算出来的.\n",
    "# 加载训练好的模型\n",
    "selfmodel = word2vec.Word2Vec.load(ModelDir+model_output)\n",
    "# 输出某个词的向量\n",
    "selfmodel.wv['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selfmodel.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "selfmodel.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "selfmodel.similarity('woman', 'man')\n",
    "#输出0.73723527 \n",
    "selfmodel.wv['computer'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
